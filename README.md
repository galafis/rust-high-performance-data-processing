# High-Performance Data Processing with Rust

![Rust](https://img.shields.io/badge/Rust-000000?style=for-the-badge&logo=rust&logoColor=white)
![Mermaid](https://img.shields.io/badge/Diagrams-Mermaid-orange?style=for-the-badge&logo=mermaid)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://github.com/galafis/rust-high-performance-data-processing/actions/workflows/rust.yml/badge.svg)](https://github.com/galafis/rust-high-performance-data-processing/actions/workflows/rust.yml)
![Tests](https://img.shields.io/badge/tests-13%20passing-brightgreen)
![Coverage](https://img.shields.io/badge/coverage-high-brightgreen)

---

## ğŸ‡§ğŸ‡· Processamento de Dados de Alta Performance com Rust

Este repositÃ³rio explora e demonstra **tÃ©cnicas e implementaÃ§Ãµes para processamento de dados de alta performance utilizando a linguagem Rust**. O foco Ã© em como aproveitar a seguranÃ§a de memÃ³ria, o controle de baixo nÃ­vel e a velocidade de execuÃ§Ã£o de Rust para construir soluÃ§Ãµes eficientes para ingestÃ£o, transformaÃ§Ã£o e anÃ¡lise de grandes volumes de dados. Ã‰ ideal para **engenheiros de dados, desenvolvedores de sistemas e pesquisadores** que buscam otimizar pipelines de dados crÃ­ticas e aplicaÃ§Ãµes que exigem mÃ¡xima performance.

### ğŸ¯ Objetivo

O principal objetivo deste projeto Ã© **fornecer exemplos prÃ¡ticos, benchmarks e tutoriais detalhados** sobre como utilizar Rust para tarefas de processamento de dados. SerÃ£o abordados tÃ³picos como manipulaÃ§Ã£o de dados em memÃ³ria, serializaÃ§Ã£o/desserializaÃ§Ã£o eficiente, integraÃ§Ã£o com bibliotecas como Apache Arrow e Polars, e estratÃ©gias para paralelizaÃ§Ã£o e concorrÃªncia.

### âœ¨ Destaques

- **Performance Extrema**: DemonstraÃ§Ãµes de como Rust oferece performance comparÃ¡vel a C/C++ para operaÃ§Ãµes intensivas em dados.
- **SeguranÃ§a de MemÃ³ria**: Exemplos que ilustram como o sistema de *ownership* e *borrowing* de Rust previne erros comuns de memÃ³ria em tempo de compilaÃ§Ã£o.
- **ConcorrÃªncia Eficiente**: UtilizaÃ§Ã£o de recursos nativos de Rust para construir pipelines de dados concorrentes e paralelas de forma segura e performÃ¡tica.
- **IntegraÃ§Ã£o com Ecossistema de Dados**: Como integrar Rust com ferramentas e formatos de dados populares como CSV, JSON e Parquet.
- **CÃ³digo Profissional**: Exemplos de cÃ³digo bem estruturados, seguindo as melhores prÃ¡ticas da indÃºstria, com foco em clareza, eficiÃªncia e documentaÃ§Ã£o interna.
- **DocumentaÃ§Ã£o Completa**: Cada exemplo Ã© acompanhado de documentaÃ§Ã£o detalhada, benchmarks e casos de uso prÃ¡ticos para facilitar a compreensÃ£o e a aplicaÃ§Ã£o.
- **Testes Abrangentes**: MÃ³dulos de cÃ³digo validados atravÃ©s de testes unitÃ¡rios, de integraÃ§Ã£o e de documentaÃ§Ã£o, garantindo a robustez e a confiabilidade das implementaÃ§Ãµes.
- **Abordagem DidÃ¡tica**: ConteÃºdo apresentado de forma didÃ¡tica, ideal para aprendizado e referÃªncia, com explicaÃ§Ãµes claras sobre os conceitos e a aplicaÃ§Ã£o prÃ¡tica.
- **Processamento AvanÃ§ado de CSV**: Um mÃ³dulo completo (`csv_processing`) demonstrando o processamento eficiente de arquivos CSV, utilizando o dataset do Titanic como exemplo prÃ¡tico.

---

## ğŸ‡¬ğŸ‡§ High-Performance Data Processing with Rust

This repository explores and demonstrates **techniques and implementations for high-performance data processing using the Rust programming language**. The focus is on how to leverage Rust's memory safety, low-level control, and execution speed to build efficient solutions for ingesting, transforming, and analyzing large volumes of data. It is ideal for **data engineers, system developers, and researchers** looking to optimize critical data pipelines and applications that demand maximum performance.

### ğŸ¯ Objective

The main objective of this project is to **provide practical examples, benchmarks, and detailed tutorials** on how to use Rust for data processing tasks. Topics covered include in-memory data manipulation, efficient serialization/deserialization, integration with libraries like Apache Arrow and Polars, and strategies for parallelization and concurrency.

### âœ¨ Highlights

- **Extreme Performance**: Demonstrations of how Rust offers performance comparable to C/C++ for data-intensive operations.
- **Memory Safety**: Examples illustrating how Rust's ownership and borrowing system prevents common memory errors at compile time.
- **Efficient Concurrency**: Utilization of native Rust features to build concurrent and parallel data pipelines safely and performantly.
- **Data Ecosystem Integration**: How to integrate Rust with popular data tools and formats like CSV, JSON, and Parquet.
- **Professional Code**: Well-structured code examples, following industry best practices, with a focus on clarity, efficiency, and internal documentation.
- **Complete Documentation**: Each example is accompanied by detailed documentation, benchmarks and practical use cases to facilitate understanding and application.
- **Comprehensive Testing**: Code modules validated through unit tests, integration tests, and documentation tests, ensuring the robustness and reliability of the implementations.
- **Didactic Approach**: Content presented didactically, ideal for learning and reference, with clear explanations of concepts and practical application.
- **Advanced CSV Processing**: A complete module (`csv_processing`) demonstrating efficient CSV file processing, using the Titanic dataset as a practical example.

### ğŸ“Š Visualization

![Rust Data Processing Performance](images/rust_high_performance_data_processing.png)

*Comparative performance chart of Rust in data processing tasks relative to other languages.*

---

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    A[Main Application] --> B[In-Memory Processing]
    A --> C[CSV Processing Module]
    B --> D[DataRecord Vector]
    B --> E[Iterator-based Processing]
    C --> F[Serde Deserialization]
    C --> G[Statistical Analysis]
    F --> H[TitanicPassenger]
    H --> G
    E --> I[Aggregated Results]
    G --> I
```

*High-level architecture diagram showing the data flow through the application.*

---

## ğŸ“ Repository Structure

```
rust-high-performance-data-processing/
â”œâ”€â”€ src/                    # Rust source code
â”‚   â”œâ”€â”€ main.rs            # Main application entry point
â”‚   â”œâ”€â”€ lib.rs             # Library public API
â”‚   â””â”€â”€ csv_processing/    # CSV processing module
â”‚       â””â”€â”€ mod.rs         # CSV module implementation
â”œâ”€â”€ tests/                  # Integration tests
â”‚   â””â”€â”€ integration_tests.rs
â”œâ”€â”€ benches/                # Performance benchmarks
â”‚   â””â”€â”€ data_processing_benchmark.rs
â”œâ”€â”€ data/                   # Sample datasets
â”‚   â””â”€â”€ titanic.csv        # Titanic passenger dataset
â”œâ”€â”€ images/                 # Images and graphics for documentation
â”‚   â”œâ”€â”€ rust_high_performance_data_processing.png
â”‚   â””â”€â”€ rust_benchmark.png
â”œâ”€â”€ diagrams/               # Mermaid diagrams
â”‚   â””â”€â”€ rust_data_processing.mmd
â”œâ”€â”€ docs/                   # Additional documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md    # Architecture and design documentation
â”‚   â””â”€â”€ PERFORMANCE_GUIDE.md  # Performance optimization guide
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ build_and_test.sh  # Build and test automation
â”‚   â””â”€â”€ run_benchmarks.sh  # Benchmark execution script
â”œâ”€â”€ .github/                # GitHub configuration
â”‚   â””â”€â”€ workflows/         # CI/CD workflows
â”‚       â””â”€â”€ rust.yml       # Rust CI pipeline
â”œâ”€â”€ Cargo.toml             # Project configuration and dependencies
â”œâ”€â”€ Cargo.lock             # Locked dependency versions
â”œâ”€â”€ LICENSE                # MIT license
â””â”€â”€ README.md              # This file
```

---

## ğŸš€ Getting Started

### Prerequisites

- Rust 1.70 or higher (install from [rustup.rs](https://rustup.rs/))
- Git

### Installation

Clone the repository and navigate to the project directory:

```bash
git clone https://github.com/galafis/rust-high-performance-data-processing.git
cd rust-high-performance-data-processing
```

### Building the Project

Build the project in release mode for optimal performance:

```bash
cargo build --release
```

### Running the Application

Execute the main application:

```bash
cargo run --release
```

Expected output:
```
===========================================
Rust High-Performance Data Processing
===========================================
Processed 1000000 records
Average value: 749999.25
Time elapsed: ~27ms

Sample records:
  Record ID: 0, Value: 0.00
  Record ID: 1, Value: 1.50
  Record ID: 2, Value: 3.00
===========================================

Iniciando exemplo de processamento CSV avanÃ§ado (Titanic)...

--- AnÃ¡lise de Dados do Titanic ---
Total de passageiros processados: 891
Passageiros que sobreviveram: 342
Taxa de sobrevivÃªncia: 38.38%
Passageiros masculinos: 577
Passageiros femininos: 314
------------------------------------
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
cargo test
```

This runs:
- **Unit tests** (8 tests in library modules)
- **Integration tests** (5 tests in `tests/` directory)
- **Documentation tests** (2 tests from code examples)

**Total: 13 passing tests** âœ“

### Run Specific Test Suites

```bash
# Run only unit tests
cargo test --lib

# Run only integration tests
cargo test --test integration_tests

# Run only documentation tests
cargo test --doc
```

### Test Coverage

Run tests with verbose output:

```bash
cargo test -- --nocapture
```

---

## ğŸ“Š Benchmarks

Run performance benchmarks using Criterion:

```bash
cargo bench
```

Or use the provided script:

```bash
./scripts/run_benchmarks.sh
```

Benchmark results are saved to `target/criterion/` with detailed HTML reports.

### Benchmark Categories

1. **In-Memory Processing**: Tests data processing at different scales (100, 1K, 10K, 100K records)
2. **CSV Analysis**: Measures CSV parsing and analysis performance
3. **Data Generation**: Benchmarks record creation and vector allocation

---

## ğŸ“š Documentation

### Generate Documentation

Generate and view the project documentation:

```bash
cargo doc --no-deps --open
```

### Additional Docs

- [Architecture Guide](docs/ARCHITECTURE.md) - System design and architecture
- [Performance Guide](docs/PERFORMANCE_GUIDE.md) - Optimization techniques and best practices

---

## ğŸ”§ Development

### Code Quality

Check code formatting:

```bash
cargo fmt --check
```

Auto-format code:

```bash
cargo fmt
```

Run linter:

```bash
cargo clippy -- -D warnings
```

### Automated Build and Test

Use the comprehensive build script:

```bash
./scripts/build_and_test.sh
```

This script:
1. Cleans previous builds
2. Checks code formatting
3. Runs clippy
4. Builds in debug mode
5. Runs all tests
6. Builds in release mode
7. Executes the application

---

## ğŸ“ˆ Performance Characteristics

| Operation | Dataset Size | Average Time | Memory Usage |
|-----------|--------------|--------------|--------------|
| In-Memory Processing | 100K records | ~1-2 ms | ~2.4 MB |
| In-Memory Processing | 1M records | ~27 ms | ~24 MB |
| CSV Parsing (Titanic) | 891 records | ~1-2 ms | Minimal |

*Benchmarks run on a typical modern CPU in release mode.*

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues, pull requests, or suggest improvements.

### Guidelines

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`cargo test`)
5. Run formatting (`cargo fmt`)
6. Run clippy (`cargo clippy`)
7. Commit your changes (`git commit -m 'Add amazing feature'`)
8. Push to the branch (`git push origin feature/amazing-feature`)
9. Open a Pull Request

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- The Rust community for excellent tools and libraries
- [csv-rs](https://github.com/BurntSushi/rust-csv) for efficient CSV parsing
- [Serde](https://serde.rs/) for powerful serialization
- [Criterion](https://github.com/bheisler/criterion.rs) for statistical benchmarking

---

**Author:** Gabriel Demetrios Lafis  
**Year:** 2025  
**Contact:** [GitHub](https://github.com/galafis)

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

---

**Happy Data Processing with Rust! ğŸ¦€**

